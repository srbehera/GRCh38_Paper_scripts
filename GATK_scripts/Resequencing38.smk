workflow_vars = {
    #'bwa_0_7_15': '/hgsc_software/bwa/bwa-0.7.15/bwa',
    'dbsnp_138': '/stornext/snfs5/next-gen/HgvData/Databases/dbSNP/Homo_sapiens_assembly38.dbsnp138.vcf.gz',
    'event_archive_directory': 'archive',
    'event_flags_directory': 'flags',
    'event_hgv_directory': '.hgv',
    'event_reports_directory': 'reports',
    'event_tmp_directory': 'tmp',
    'event_variants_directory': 'variants',
    'gatk_3_6': '/hgsc_software/gatk/GenomeAnalysisTK-3.6/GenomeAnalysisTK.jar',
    'java_1_8': '/hgsc_software/java/jdk1.8.0_74/bin/java',
    'known_indels_ccdg': '/stornext/snfs5/next-gen/HgvData/Databases/ccdg_known_indels/Homo_sapiens_assembly38.known_indels.vcf.gz',
    'known_indels_mills_1000g_ccdg': '/stornext/snfs5/next-gen/HgvData/Databases/Mills_InDels/Mills_and_1000G_gold_standard.indels.hg38.vcf.gz',
    'picard_2_6_0': '/hgsc_software/picard/picard-tools-2.6.0/picard.jar',
    'sambamba_0_6_7': '/hgsc_software/sambamba/sambamba_v0.6.7/sambamba',
    #'samblaster_0_1_24': '/hgsc_software/samblaster/samblaster-v.0.1.24/samblaster',
    #'samtools_1_9': '/hgsc_software/samtools/samtools-1.9/bin/samtools',
    'tmp_dir': '/space1/tmp/$PBS_JOBID'
}

from pathlib import Path
from shutil import move, rmtree

from hgv.util.snakemake_utils import *

event_hgv_dir = Path(config["event"]["path"], workflow_vars["event_hgv_directory"], config["timestamp"])
event_flags_dir = Path(config["event"]["path"], workflow_vars["event_hgv_directory"], workflow_vars["event_flags_directory"])
event_tmp_dir = Path(config["event"]["path"], workflow_vars["event_tmp_directory"])
event_reports_dir = Path(config["event"]["path"], workflow_vars["event_reports_directory"])
event_variants_dir = Path(config["event"]["path"], workflow_vars["event_variants_directory"])
event_archive_dir = Path(config["event"]["path"], workflow_vars["event_archive_directory"])
event_snakemake_dir = Path(config["event"]["path"], ".snakemake")

localrules: all

#rule BwaMem:
#    input:
#        bwa=Path(workflow_vars["bwa_0_7_15"]),
#        samblaster=Path(workflow_vars["samblaster_0_1_24"]),
#        samtools=Path(workflow_vars["samtools_1_9"]),
#        reference=Path(config["event"]["reference"]),
#        fastq_read1=Path(config["event"]["fastq_read1"]),
#        fastq_read2=Path(config["event"]["fastq_read2"]),
#    output:
#        bam=Path(event_tmp_dir, "{event_id}.initial.bam".format(event_id=config["event"]["event_id"])),
#    params:
#        rg_string=config["event"]["rg_string"],
#        tmp_dir=str(Path(workflow_vars["tmp_dir"])),
#        chunk_size="100000000",
#    resources:
#        ppn=scale_resource(config, 8, 1, 32),
#        mem_mb=scale_resource(config, 8, 1, 32)*4000,
#        walltime_hrs=168,
#    shell:
#        """\
#{input.bwa} mem \
#    -K {params.chunk_size} \
#    -Y \
#    -t {resources.ppn} \
#    -R '{params.rg_string}' \
#    {input.reference} \
#    {input.fastq_read1} \
#    {input.fastq_read2} | \
#{input.samblaster} \
#    --addMateTags \
#    -a | \
#{input.samtools} view \
#    -1 \
#    -o {output.bam} -"""

rule PicardMarkdups:
    input:
        sambamba=Path(workflow_vars["sambamba_0_6_7"]),
        java=Path(workflow_vars["java_1_8"]),
        picard=Path(workflow_vars["picard_2_6_0"]),
        reference=Path(config["event"]["reference"]),
        #bam=Path(event_tmp_dir, "{event_id}.initial.bam".format(event_id=config["event"]["event_id"])),
        bam=Path(config["event"]["extracted_reads_bam"]),
    output:
        bam=Path(event_tmp_dir, "{event_id}.PicardMarkdups.bam".format(event_id=config["event"]["event_id"])),
        bai=Path(event_tmp_dir, "{event_id}.PicardMarkdups.bam.bai".format(event_id=config["event"]["event_id"])), # note: index generated by sambamba
        metrics_file=Path(event_reports_dir, "PicardMarkdups", "{event_id}.metrics.txt".format(event_id=config["event"]["event_id"])),
    params:
        bam_compression_level=4,
        tmp_bam_compression_level=1,
        tmp_dir=str(Path(workflow_vars["tmp_dir"])),
        event_id=config["event"]["event_id"],
        tmp_namesort_bam=str(Path(workflow_vars["tmp_dir"], "{event_id}.PicardMarkdups.namesort.bam".format(event_id=config["event"]["event_id"]))),
        tmp_markdups_bam=str(Path(workflow_vars["tmp_dir"], "{event_id}.PicardMarkdups.markdups.bam".format(event_id=config["event"]["event_id"]))),
    resources:
        ppn=scale_resource(config, 8, 1, 32),
        mem_mb=scale_resource(config, 8, 1, 32)*6000,
        java_xmx_mb=scale_resource(config, 8, 1, 32)*6000-1000,
        sambamba_mem_mb=scale_resource(config, 8, 1, 32)*6000-1000,
        walltime_hrs=168,
    shell:
        """\
{input.sambamba} sort \
    --sort-by-name \
    --memory-limit={resources.sambamba_mem_mb}M \
    --nthreads={resources.ppn} \
    --compression-level={params.tmp_bam_compression_level} \
    --tmpdir={params.tmp_dir} \
    --out={params.tmp_namesort_bam} {input.bam}; \
{input.java} -Xmx{resources.java_xmx_mb}m -jar {input.picard} MarkDuplicates \
    TMP_DIR={params.tmp_dir} \
    I={params.tmp_namesort_bam} \
    O={params.tmp_markdups_bam} \
    ASSUME_SORT_ORDER=queryname \
    QUIET=true \
    CREATE_INDEX=false \
    METRICS_FILE={output.metrics_file} \
    COMPRESSION_LEVEL={params.tmp_bam_compression_level}; \
{input.sambamba} sort \
    --nthreads={resources.ppn} \
    --memory-limit={resources.sambamba_mem_mb}M \
    --compression-level={params.bam_compression_level} \
    --tmpdir={params.tmp_dir} \
    --out={output.bam} \
    {params.tmp_markdups_bam}"""
#rm -f {input.bam}

rule GatkBaseRecalibrator:
    input:
        java=Path(workflow_vars["java_1_8"]),
        gatk=Path(workflow_vars["gatk_3_6"]),
        dbsnp=Path(workflow_vars["dbsnp_138"]),
        known_indels_ccdg=Path(workflow_vars["known_indels_ccdg"]),
        known_indels_mills_1000g_ccdg=Path(workflow_vars["known_indels_mills_1000g_ccdg"]),
        reference=Path(config["event"]["reference"]),
        bam=rules.PicardMarkdups.output.bam,
        #interval_list=Path(config["event"]["extracted_reads_interval_list"]),
    output:
        recal_file=Path(event_tmp_dir, "{event_id}.GatkBaseRecalibrator.file".format(event_id=config["event"]["event_id"])),
    params:
        downsample_to_fraction=".1",
        intervals_opt=expand("--intervals chr{chrnum}", chrnum=list(range(1, 23))),
    resources:
        ppn=scale_resource(config, 4, 1, 16),
        mem_mb=scale_resource(config, 4, 1, 16)*6000,
        java_xmx_mb=scale_resource(config, 4, 1, 16)*6000-1000,
        walltime_hrs=168,
    shell:
        """\
{input.java} -Xmx{resources.java_xmx_mb}m -jar {input.gatk} --analysis_type BaseRecalibrator \
    --input_file {input.bam} \
    --out {output.recal_file} \
    --reference_sequence {input.reference} \
    --num_cpu_threads_per_data_thread {resources.ppn} \
    --knownSites {input.dbsnp} \
    --knownSites {input.known_indels_ccdg} \
    --knownSites {input.known_indels_mills_1000g_ccdg} \
    --downsample_to_fraction {params.downsample_to_fraction} \
    {params.intervals_opt}"""
#--intervals {input.interval_list}

rule GatkPrintReads:
    input:
        java=Path(workflow_vars["java_1_8"]),
        gatk=Path(workflow_vars["gatk_3_6"]),
        reference=Path(config["event"]["reference"]),
        bam=rules.PicardMarkdups.output.bam,
        recal_file=rules.GatkBaseRecalibrator.output.recal_file,
        interval_list=Path(config["event"]["extracted_reads_interval_list"]),
    output:
        bam=Path(event_tmp_dir, "{event_id}.GatkPrintReads.bam".format(event_id=config["event"]["event_id"])),
        bai=Path(event_tmp_dir, "{event_id}.GatkPrintReads.bai".format(event_id=config["event"]["event_id"])),
    params:
        downsampling_type="NONE",
        global_q_score_prior="-1.0",
        preserve_qscores_less_than=6,
        static_quantized_quals_opt=expand("--static_quantized_quals {qual}", qual=[10, 20, 30]),
        bam_compression_level=4,
    resources:
        ppn=scale_resource(config, 4, 1, 16),
        mem_mb=scale_resource(config, 4, 1, 16)*6000,
        java_xmx_mb=scale_resource(config, 4, 1, 16)*6000-1000,
        walltime_hrs=168,
    shell:
        """\
{input.java} -Xmx{resources.java_xmx_mb}m -jar {input.gatk} --analysis_type PrintReads \
    --input_file {input.bam} \
    --out {output.bam} \
    --reference_sequence {input.reference} \
    --num_cpu_threads_per_data_thread {resources.ppn} \
    --BQSR {input.recal_file} \
    --globalQScorePrior {params.global_q_score_prior} \
    --preserve_qscores_less_than {params.preserve_qscores_less_than} \
    {params.static_quantized_quals_opt} \
    --disable_indel_quals \
    --downsampling_type {params.downsampling_type} \
    --intervals {input.interval_list} \
    --bam_compression {params.bam_compression_level}; \
rm -f {input.bam}"""

rule GatkHaplotypeCallerVcf:
    input:
        java=Path(workflow_vars["java_1_8"]),
        gatk=Path(workflow_vars["gatk_3_6"]),
        reference=Path(config["event"]["reference"]),
        bam=rules.GatkPrintReads.output.bam,
        interval_list=Path(config["event"]["extracted_reads_interval_list"]),
    output:
        gvcf=Path(event_variants_dir, "HaplotypeCaller", "{event_id}.vcf.gz".format(event_id=config["event"]["event_id"])),
    params:
        max_alternate_alleles=3,
    resources:
        ppn=16,
        mem_mb=96000,
        java_xmx_mb=95000,
        walltime_hrs=168,
    shell:
        """\
{input.java} -Xmx{resources.java_xmx_mb}m -jar {input.gatk} --analysis_type HaplotypeCaller \
    --num_cpu_threads_per_data_thread {resources.ppn} \
    --input_file {input.bam} \
    --out {output.gvcf} \
    --max_alternate_alleles {params.max_alternate_alleles} \
    --intervals {input.interval_list} \
    --reference_sequence {input.reference}"""

rule GatkHaplotypeCallerGvcf:
    input:
        java=Path(workflow_vars["java_1_8"]),
        gatk=Path(workflow_vars["gatk_3_6"]),
        reference=Path(config["event"]["reference"]),
        bam=rules.GatkPrintReads.output.bam,
        interval_list=Path(config["event"]["extracted_reads_interval_list"]),
    output:
        gvcf=Path(event_variants_dir, "HaplotypeCaller", "{event_id}.g.vcf.gz".format(event_id=config["event"]["event_id"])),
    params:
        max_alternate_alleles=3,
    resources:
        ppn=16,
        mem_mb=96000,
        java_xmx_mb=95000,
        walltime_hrs=168,
    shell:
        """\
{input.java} -Xmx{resources.java_xmx_mb}m -jar {input.gatk} --analysis_type HaplotypeCaller \
    --num_cpu_threads_per_data_thread {resources.ppn} \
    --input_file {input.bam} \
    --out {output.gvcf} \
    --reference_sequence {input.reference} \
    --max_alternate_alleles {params.max_alternate_alleles} \
    --intervals {input.interval_list} \
    --emitRefConfidence GVCF"""

rule all:
    input:
        rules.GatkPrintReads.output,
        rules.GatkHaplotypeCallerVcf.output,
        rules.GatkHaplotypeCallerGvcf.output,
    output:
        #final_bam=Path("{bam_prefix}.bam".format(bam_prefix=config["event"]["bam_prefix"])),
        #final_bai=Path("{bam_prefix}.bam.bai".format(bam_prefix=config["event"]["bam_prefix"])),
        flag=touch(Path(event_flags_dir, "all.done")),
    params:
        hgv_config_pkl=str(Path(event_hgv_dir, "hgvconfig.pkl")),
        event_id=config["event"]["event_id"],
        event_dir=config["event"]["path"],
    resources:
        ppn=1,
        mem_mb=2000,
        walltime_hrs=24,
    run:
        #move(rules.GatkPrintReads.output.bam, output.final_bam)
        #move(rules.GatkPrintReads.output.bai, output.final_bai)

        preserve_temp_dir = "preserve_temp_dir" in config["pipeline"] and config["pipeline"]["preserve_temp_dir"]

        if not preserve_temp_dir and event_tmp_dir.exists():
            rmtree(event_tmp_dir)

        if event_snakemake_dir.exists():
            add_group_permissions(event_snakemake_dir)
